---
title: "Premier League Coaching Changes"
title-slide-attributes:
  data-background-color: "#486790"
format: 
  revealjs:
    smaller: true
    slide-number: c/t
    self-contained: true
highlight-style: github
---
### Background Info

Why this project?  

- Coaching changes are common in football, often seen as a way to improve performance.  
- However, does replacing a coach actually improve results?  

---

### Introduction

- **Objective**: Estimate the causal effect of mid-season coaching changes on match performance in the 2023–2024 Premier League season.
- **Approach**: Use match outcomes, coaching history, and techniques learned in class to infer causality.

---

### DAG

![](../figures/FinalDag.png){fig-align="center"}

---

##### DAG Expanded

- The DAG encodes assumptions about how different factors influence match performance (Points) and each other.
- Coach affects Points indirectly through Team_Strength. This reflects how coaching influences performance over time (by developing players) rather than immediate match-level decisions.
- Team_Strength then affects Points, making it a **mediator** in the causal path from Coach to Points.
- Opponent_Strength affects Points because tougher opponents reduce the chance of earning points. It is treated as **external and fixed** for each match.
- Team influences Coach, Venue, Team_Strength, and Points because each club’s structure, resources, and identity affect hiring decisions, home-field designation, and outcomes.
- Start_Date and End_Date determine the time range when a coach is active and collectively define whether a coaching change has occurred.
- Date influences both Coach and Venue, as changes and fixtures vary across the season.

---

### Identification Strategy

- The goal is to estimate the **total effect** of a coaching change on match performance (measured by Points).
- To do this, it’s necessary to **adjust for variables that confound** the relationship between Coach and Points.

---

### Adjustment Set

Based on the DAG, the following variables satisfy the backdoor criterion for identifying the causal effect of Coach on Points:

- Date: captures seasonal trends and timing patterns that influence both coaching changes and match performance.
- Venue: home vs. away affects outcomes and is determined by the fixture list, not the coach.
- Team: accounts for structural differences across clubs (budget, talent, expectations) that affect both coach decisions and results.

---

### Why Not Include Other Variables?

- Team_Strength and Opponent_Strength are **mediators**, lying on the causal path from Coach to Points, so adjusting for them would block part of the effect being estimated.
- Start_Date and End_Date define the treatment (i.e., whether a coaching change occurred) and should not be conditioned on.

---

### EDA

- Before modeling, I explored the data to understand how teams performed, how coaching changes were distributed, and how match context varied.

---

### Points Distribution

- The **points distribution** confirms the expected trimodal pattern (0, 1, and 3 points), aligning with how points are awarded in the Premier League.
- Wins and losses are common; draws are less frequent.

![](../figures/distribution_of_points.png){fig-align="center"}

---

### Squad Value Distributions

- **Team strength and opponent strength** distributions show a right-skewed pattern.
- Most teams fall in a mid-range, but a few top clubs are valued significantly higher.
- These were useful for EDA but treated as mediators in the causal model.

![](../figures/distribution_of_team_strength.png){fig-align="center"}

---

![](../figures/distribution_of_opponent_strength.png){fig-align="center"}

---

### Average Points by Venue

- Teams earn **more points at home** on average, confirming a home-field advantage.

![](../figures/avg_points_by_venue.png){fig-align="center"}

---

### Correlation Heatmap

- **Points**:
  - Increase with team strength (~0.20)
  - Decrease with opponent strength (~-0.28)
  - Slight negative relationship with coaching changes (~-0.10)

![](../figures/correlation_heatmap.png){fig-align="center"}

---

### Number of Matches With and Without a Coaching Change

- About **500 matches occurred without a coaching change**.
- Roughly **260 followed a change**, making it frequent enough to study but not balanced.

![](../figures/num_of_matches_with_without_CC.png){fig-align="center"}

---

### Average Points by Timing of Coaching Change

- Teams that changed coaches **mid-season performed worse on average**.
- Teams with **no coaching change** or a **preseason change** earned more points.
- Suggests mid-season changes may be reactive and not performance-improving.

![](../figures/point_dist_by_change.png){fig-align="center"}

---

### Estimating the Causal Effect

- To estimate the effect of coaching changes on performance, I built a model that accounts for when the match was played, where it was played, and which team was involved.

---

### What the Model Included

The model considered:

- CoachChange: whether the team had recently changed coaches (**this is the main variable of interest**)
- Date: when in the season the match was played
- Venue: whether the team was playing home or away
- Team: the team name, to account for differences in team quality (like Manchester United vs. Ipswich Town)

---

### What the Model Found

- The average effect of a coaching change was about **−0.94 points per match**.
- This means teams that changed coaches tended to earn **slightly fewer points** than those that didn’t.
- However, the model also found that the true effect could range **anywhere from −4.41 to +2.01 points**.

---

### How to Interpret That

- The range above is called a **credible interval**. It tells us where the true effect is likely to fall based on the data.
- Because the range **includes 0**, we can’t say for sure that coaching changes help or hurt, the data just doesn’t give a clear answer.
- In simple terms: **coaching changes don’t seem to consistently improve results**, and in some cases, they may be a response to deeper problems the coach can’t fix.

---

### Conclusion / Next Steps

- Coaching changes are often seen as a quick fix, but this analysis suggests they don’t reliably lead to better results.
- The estimated effect of a coaching change on match performance was small and uncertain, with a credible interval that included zero.
- Overall, coaching changes may be **a reaction to poor performance**, rather than a consistent solution.

---

### Next Steps

- **Expand the scope**: Analyze multiple seasons to see if these patterns persist over time.
- Consider **Matching Strategy**
- Consider **Regression Discontinuity** and **Marginal Effects**

---
# Thank You
